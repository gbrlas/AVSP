{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linesCodeAnalyzer = None\n",
    "with open('CodeJamCrawler/dataset_csvs/code_analyzer_out_sorted.csv', 'r') as f:\n",
    "    linesCodeAnalyzer = f.readlines()\n",
    "    \n",
    "codeAnalyzerHeader = linesCodeAnalyzer[0]\n",
    "linesCodeAnalyzer = linesCodeAnalyzer[1:]\n",
    "\n",
    "linesCccc = None\n",
    "with open('CodeJamCrawler/dataset_csvs/cccc_cleared.csv', 'r') as f:\n",
    "    linesCccc = f.readlines()\n",
    "    \n",
    "linesASTExtractor = None\n",
    "with open('ASTExtractor/ast_features.csv', 'r') as f:\n",
    "    linesASTExtractor = f.readlines()\n",
    "\n",
    "ASTExtractorHeader = linesASTExtractor[0]\n",
    "    \n",
    "ccccHeader = linesCccc[0]\n",
    "linesCccc = linesCccc[1:]\n",
    "linesASTExtractor = linesASTExtractor[1:]\n",
    "\n",
    "print (linesCodeAnalyzer[0])\n",
    "print (linesCccc[0])\n",
    "print (ASTExtractorHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file_name(line, splitter):\n",
    "    entries = line.split(splitter)\n",
    "    file_name, file_ext = os.path.splitext(entries[0])\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (get_file_name(linesCodeAnalyzer[0], ','))\n",
    "print (get_file_name(linesCccc[0], ';'))\n",
    "print (get_file_name(linesASTExtractor[0], ';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing all the entries that don't exist in outputCleared\n",
    "cpp_files_only = []\n",
    "for i in range(len(linesCodeAnalyzer)):\n",
    "    if i % 1000 == 0:\n",
    "        print (i)\n",
    "        \n",
    "    other = linesCodeAnalyzer[i]\n",
    "    file_name_other = get_file_name(other, ',')\n",
    "    \n",
    "    found = False\n",
    "    for cleared in linesCccc:\n",
    "        file_name_cleared = get_file_name(cleared, ';')\n",
    "        if file_name_cleared == file_name_other:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        continue\n",
    "    cpp_files_only.append(file_name_other)\n",
    "    \n",
    "print (\"Number of cpp files:\", len(cpp_files_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (cpp_files_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Code analyzer output:', codeAnalyzerHeader)\n",
    "print ('CCCC analyzer output:', ccccHeader)\n",
    "print ('ASTExtractor output:', ASTExtractorHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_cccc(cccc_line):\n",
    "    name = get_file_name(cccc_line, ';')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name\n",
    "    features = []\n",
    "    for num in cccc_line.split(';')[1:]:\n",
    "        if num == '******' or num == '------':\n",
    "            num = '0.0'\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_code_analyzer(code_anal_line):\n",
    "    name = get_file_name(code_anal_line, ',')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name\n",
    "    features = []\n",
    "    for num in code_anal_line.split(',')[1:]:\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_ast_extractor(code_extractor_line):\n",
    "    name = get_file_name(code_extractor_line, ';')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name#.split('_')[0]\n",
    "    features = []\n",
    "    for num in code_extractor_line.split(';')[1:]:\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('cccc line vec', parse_cccc(linesCccc[0]))\n",
    "print ('code line vec', parse_code_analyzer(linesCodeAnalyzer[0]))\n",
    "print ('ast line vec', parse_ast_extractor(linesASTExtractor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpp_files_map = set()\n",
    "\n",
    "for n in cpp_files_only:\n",
    "    cpp_files_map.add(n)\n",
    "\n",
    "src_file_vec_map = {}\n",
    "\n",
    "for cccc_line in linesCccc:\n",
    "    name, features, res = parse_cccc(cccc_line)\n",
    "    if name not in src_file_vec_map and name in cpp_files_map:\n",
    "        src_file_vec_map[name] = [(features,res)]\n",
    "    elif name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "\n",
    "for code_anal_line in linesCodeAnalyzer:\n",
    "    name, features, res = parse_code_analyzer(code_anal_line)\n",
    "    if name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "\n",
    "i = 0\n",
    "for ast_line in linesASTExtractor:\n",
    "    name, features, res = parse_ast_extractor(ast_line)\n",
    "    if name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "        i += 1\n",
    "        \n",
    "print (i, len(cpp_files_only), len(cpp_files_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author_name(file_name):\n",
    "    return file_name.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vec_map = {}\n",
    "\n",
    "for key in src_file_vec_map.keys():\n",
    "    v1 = src_file_vec_map[key][0]\n",
    "    v2 = src_file_vec_map[key][1]\n",
    "    v3 = src_file_vec_map[key][2]\n",
    "    author = get_author_name(key)\n",
    "    \n",
    "    if author not in src_vec_map.keys():\n",
    "        src_vec_map[author] = [(v1,v2,v3)]\n",
    "    else:\n",
    "        src_vec_map[author].append((v1, v2, v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(src_vec_map))\n",
    "\n",
    "sample_count = 0\n",
    "for k in src_vec_map:\n",
    "    sample_count += len(src_vec_map[k])\n",
    "    \n",
    "print (\"Number of samples: \", sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_dataset(src_vec_map):\n",
    "    dataset_list = []\n",
    "    \n",
    "    for k in src_vec_map:\n",
    "        for i in range(len(src_vec_map[k])):\n",
    "            label = src_vec_map[k][i][0][1]\n",
    "            l1 = np.asarray(src_vec_map[k][i][0][0])\n",
    "            l2 = np.asarray(src_vec_map[k][i][1][0])\n",
    "            l3 = np.asarray(src_vec_map[k][i][2][0])\n",
    "            l = np.concatenate((l1, l2), axis=0)\n",
    "            l = np.concatenate((l, l3), axis=0)\n",
    "            dataset_list.append((l, (k, label)))\n",
    "    dataset = np.array(dataset_list)\n",
    "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(src_vec_map)\n",
    "print (dataset.shape)\n",
    "print (dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author_features_and_scores(author_name, dataset):\n",
    "    codes = []\n",
    "    scores = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        if item[1][0] == author_name:\n",
    "            codes.append(item[0])\n",
    "            scores.append(item[1][1])\n",
    "            \n",
    "    return (codes, scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_names = list(src_vec_map.keys())\n",
    "X_list = []\n",
    "\n",
    "for author in author_names:\n",
    "    codes, scores = get_author_features_and_scores(author, dataset)\n",
    "    X_list.extend(codes)\n",
    "    \n",
    "X = np.asarray(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (codeAnalyzerHeader.split(',')[1:-1], \"\\n\")\n",
    "print (ccccHeader.split(';')[1:], \"\\n\")\n",
    "print (ASTExtractorHeader.split(';')[1:-1])\n",
    "labels = ccccHeader.split(';')[1:] + codeAnalyzerHeader.split(',')[1:-1] + ASTExtractorHeader.split(';')[1:]\n",
    "\n",
    "print (\"\\nTotal label length before clearing:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def enlist_kept_cols(X, labels):\n",
    "    cnt_cols = X.shape[1]\n",
    "    X_reduced = []\n",
    "    kept_cols = []\n",
    "    \n",
    "    for i in range(cnt_cols):\n",
    "        col = X[:,i]\n",
    "        notInside = True\n",
    "        for j in range(len(X_reduced)):\n",
    "            if (col == X_reduced[j]).all():\n",
    "                notInside = False\n",
    "        \n",
    "        if notInside:\n",
    "            kept_cols.append(i)\n",
    "            X_reduced.append(col)\n",
    "\n",
    "    return kept_cols\n",
    "\n",
    "kept_cols = enlist_kept_cols(X, labels)\n",
    "print (kept_cols, \"\\n\")\n",
    "\n",
    "X_reduced = X[:, kept_cols]\n",
    "print (X_reduced.shape, \"\\n\")\n",
    "\n",
    "labels_reduced = [labels[i] for i in kept_cols]\n",
    "print (labels_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json(author_names, dataset):\n",
    "    data = {}\n",
    "    data[\"column_descriptors\"] = labels_reduced\n",
    "    data[\"author_data\"] = {}\n",
    "    for author in author_names:\n",
    "        feature_vecs, scores = get_author_features_and_scores(author, dataset)\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        feat_vecs = [fv[kept_cols].tolist() for fv in feature_vecs]\n",
    "        author_data = {}\n",
    "        author_data[\"feature_vecs\"] = feat_vecs\n",
    "        author_data[\"scores\"] = scores\n",
    "        data[\"author_data\"][author] = author_data\n",
    "        \n",
    "    with open('dataset.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "#print get_author_features_and_scores(author_names[15], dataset)[1]\n",
    "create_json(author_names, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
