{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "def load_json():\n",
    "    \n",
    "    authors = []\n",
    "    features = []\n",
    "    scores = []\n",
    "    \n",
    "    with open('dataset.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        labels = data[\"column_descriptors\"]\n",
    "        author_data = data[\"author_data\"]\n",
    "        \n",
    "        for author in author_data.keys():\n",
    "            authors.append(author)\n",
    "            features.append(author_data[author]['feature_vecs'])\n",
    "            scores_str = author_data[author]['scores']\n",
    "            scores.append(np.asarray([int(score_) for score_ in scores_str]))\n",
    "    \n",
    "    # features to numpy array\n",
    "    features = [np.asarray(feature) for feature in features]\n",
    "    \n",
    "    return labels, authors, features, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webNeat (6, 26) (6,)\n",
      "['number_of_modules', 'lines_of_code', 'lines_of_code_per_module', 'McCabes_cyclomatic_complexity', 'McCabes_cyclomatic_complexity_per_module', 'lines_of_comment', 'lines_of_comment_per_module', 'lines_of_code_per_line_of_comment', 'McCabes_cyclomatic_complexity_per_line_of_comment', 'IF4', 'IF4_per_module', 'IF4_visible', 'IF4_visible_per_module', 'IF4_concrete', 'IF4_concrete', 'rejected_lines_of_code\\n', 'Files', 'Lines', 'AVG Len', 'Code', 'Comments', 'White SP', 'Cd/Cm+WS', 'Cd/Cm', 'Cd/WS', '% Code']\n"
     ]
    }
   ],
   "source": [
    "labels, authors, features, scores = load_json()\n",
    "print (authors[0], features[0].shape, scores[0].shape)\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "1095\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "# training and test split\n",
    "import random\n",
    "cnt_train = int(0.7 * len(authors)) + 1\n",
    "\n",
    "print (cnt_train)\n",
    "\n",
    "train_authors_indices = set()\n",
    "\n",
    "while len(train_authors_indices) < cnt_train:\n",
    "    train_authors_indices.add(random.randint(0, len(authors) - 1))\n",
    "    \n",
    "test_authors_indices = set()\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if i not in train_authors_indices:\n",
    "        test_authors_indices.add(i)\n",
    "\n",
    "train_authors = [authors[i] for i in train_authors_indices]\n",
    "        \n",
    "test_authors = [authors[i] for i in test_authors_indices]\n",
    "\n",
    "print(len(train_authors))\n",
    "print(len(test_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12139, 26) (12139,) (5205, 26) (5205,)\n"
     ]
    }
   ],
   "source": [
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for index in train_authors_indices:\n",
    "    X_train_list.extend(features[index])\n",
    "    y_train_list.extend(scores[index])\n",
    "    \n",
    "for index in test_authors_indices:\n",
    "    X_test_list.extend(features[index])\n",
    "    y_test_list.extend(scores[index])\n",
    "\n",
    "X_train = np.asarray(X_train_list)\n",
    "y_train = np.asarray(y_train_list)\n",
    "X_test = np.asarray(X_test_list)\n",
    "y_test = np.asarray(y_test_list)\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y is vector of labels\n",
    "def create_labels(y):\n",
    "    y_l = np.copy(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] >= 80: \n",
    "            y_l[i] = 5\n",
    "        if y[i] >= 60 and y[i] < 80: \n",
    "            y_l[i] = 4\n",
    "        if y[i] >= 40 and y[i] < 60:\n",
    "            y_l[i] = 3\n",
    "        if y[i] >= 20 and y[i] < 40:\n",
    "            y_l[i] = 2\n",
    "        if y[i] < 20: \n",
    "            y_l[i] = 1\n",
    "    return y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # creating score labels for classifying\n",
    "    \n",
    "y_train = create_labels(y_train) \n",
    "y_test = create_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:\t 0.0001\n",
      "\tSolver:\t adam\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.288376560999\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.279538904899\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.296829971182\n",
      "\tSolver:\t lbfgs\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.276080691643\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.294524495677\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.261671469741\n",
      "\tSolver:\t sgd\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.274543707973\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.270701248799\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.286647454371\n",
      "Alpha:\t 0.001\n",
      "\tSolver:\t adam\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.284726224784\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.283189241114\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.268203650336\n",
      "\tSolver:\t lbfgs\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.282804995197\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.272430355427\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.281652257445\n",
      "\tSolver:\t sgd\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.256868395773\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.278578290106\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.263016330451\n",
      "Alpha:\t 0.01\n",
      "\tSolver:\t adam\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.293948126801\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.295485110471\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.268395773295\n",
      "\tSolver:\t lbfgs\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.272622478386\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.26916426513\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.260134486071\n",
      "\tSolver:\t sgd\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.286455331412\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.282228626321\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.274543707973\n",
      "Alpha:\t 0.1\n",
      "\tSolver:\t adam\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.269356388088\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.279923150817\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.272622478386\n",
      "\tSolver:\t lbfgs\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.299327569645\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.232660902978\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.262439961575\n",
      "\tSolver:\t sgd\n",
      "\t\tLearning rate:\t constant\tMLPC Score:\t 0.276657060519\n",
      "\t\tLearning rate:\t adaptive\tMLPC Score:\t 0.284918347743\n",
      "\t\tLearning rate:\t invscaling\tMLPC Score:\t 0.237848222863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1]\n",
    "solvers = [\"adam\", \"lbfgs\", \"sgd\"]\n",
    "learning_rates = [\"constant\", \"adaptive\", \"invscaling\"]\n",
    "\n",
    "for alpha in alphas:\n",
    "    print (\"Alpha:\\t\", alpha)\n",
    "    for solver in solvers:\n",
    "        print (\"\\tSolver:\\t\", solver)\n",
    "        for learning_rate in learning_rates:\n",
    "            print (\"\\t\\tLearning rate:\\t\", learning_rate, end=\"\\t\")\n",
    "            mlpC = MLPClassifier(alpha=alpha, batch_size='auto', learning_rate=learning_rate, learning_rate_init=0.01, power_t=0.5, shuffle=True, max_iter=500)\n",
    "            mlpC.fit(X_train, y_train)\n",
    "            print (\"MLPC Score:\\t\", mlpC.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC:\t 50 estimators\t|  gini  criterion\tScore:\t 0.609990393852\n",
      "\n",
      "\n",
      "RFC:\t 100 estimators\t|  gini  criterion\tScore:\t 0.62055715658\n",
      "\n",
      "\n",
      "RFC:\t 150 estimators\t|  gini  criterion\tScore:\t 0.608261287224\n",
      "\n",
      "\n",
      "RFC:\t 200 estimators\t|  gini  criterion\tScore:\t 0.611143131604\n",
      "\n",
      "\n",
      "RFC:\t 50 estimators\t|  entropy  criterion\tScore:\t 0.612680115274\n",
      "\n",
      "\n",
      "RFC:\t 100 estimators\t|  entropy  criterion\tScore:\t 0.616330451489\n",
      "\n",
      "\n",
      "RFC:\t 150 estimators\t|  entropy  criterion\tScore:\t 0.614409221902\n",
      "\n",
      "\n",
      "RFC:\t 200 estimators\t|  entropy  criterion\tScore:\t 0.614793467819\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "criterions = [ \"gini\", \"entropy\"]\n",
    "estimators_size = [50, 100, 150, 200]\n",
    "\n",
    "for criterion in criterions:\n",
    "    for n_estimators in estimators_size:\n",
    "        print (\"RFC:\\t\", n_estimators, \"estimators\\t| \", criterion, \" criterion\", end='\\t')\n",
    "        rfc = RandomForestClassifier(n_estimators=n_estimators, max_features='log2', criterion=criterion)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        print (\"Score:\\t\",rfc.score(X_test, y_test))\n",
    "        print (\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search...\n",
      "SVC:\t linear  kernel\t|  1  C\t\tScore:  0.265129682997\n",
      "\n",
      "\n",
      "SVC:\t linear  kernel\t|  10  C\t\t"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train = preprocessing.scale(X_train) \n",
    "X_test = preprocessing.scale(X_test)\n",
    "\n",
    "print (\"Starting grid search...\")\n",
    "\n",
    "kernels = ['linear', 'rbf']\n",
    "Cs = [1, 10, 100, 1000]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in Cs:\n",
    "        print (\"SVC:\\t\", kernel, \" kernel\\t| \", C, \" C\\t\", end=\"\\t\")\n",
    "        svc = SVC(C=C, kernel=kernel)\n",
    "        svc.fit(X_train, y_train)\n",
    "        print (\"Score: \", svc.score(X_test, y_test))\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print (clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.355916558981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "print (gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.288351486062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "print (abc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.604762783829\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators=30)\n",
    "etc.fit(X_train, y_train)\n",
    "print (etc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-ed0f812466c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-ed0f812466c7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print X.shape, y.reshape((y.shape[0], 1)).shape\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print X.shape, y.reshape((y.shape[0], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset = X#np.concatenate((X, y.T), axis=1)\n",
    "\n",
    "dataset = np.concatenate((X,y[:,None]),axis=1)\n",
    "#print X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "#sns.pairplot(df)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
